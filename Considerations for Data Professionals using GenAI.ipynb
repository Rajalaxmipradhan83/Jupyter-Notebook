{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "Considerations for Data Professionals using GenAI",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Exercise 1:\nYou are developing a generative AI system that creates personalized content recommendations for users. The system seems to consistently recommend content that aligns with certain cultural and demographic biases.\n\nUsers from diverse backgrounds are expressing concern about the lack of transparency and fairness in the recommendations.\n\nHow do you maintain transparency and fairness in your AI system?\n\nTo maintain transparency and fairness in the AI-powered content recommendation system, I would take the following approach:\n\n1. Identify and Assess Bias\nConduct a bias audit on training data and recommendation models.\nUse fairness assessment tools like IBM Fairness 360 and Holistic AI Library to measure bias in recommendations.\n2. Improve Data Diversity\nEnsure the training dataset is representative of different cultural, demographic, and behavioral groups.\nRegularly update and retrain the model with diverse and balanced datasets.\n3. Implement Explainability & Transparency Features\nDevelop an explainable AI (XAI) framework to show why a recommendation was made.\nProvide users with insight into key influencing factors, such as past interactions, preferences, or popular trends.\n4. Introduce Fairness Constraints in the Algorithm\nImplement fairness-aware recommendation algorithms that balance personalization with diversity.\nUse regularization techniques to reduce over-reliance on biased patterns in training data.\n5. Establish a User Feedback Loop\nAllow users to report biased recommendations and adjust their content preferences.\nContinuously analyze feedback and adjust the recommendation model accordingly.\n6. Continuous Monitoring & Evaluation\nConduct regular fairness audits to ensure recommendation diversity and mitigate emerging biases.\nUse A/B testing to compare the fairness of different recommendation strategies.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Exercise 2\nYour company has deployed a chatbot powered by generative AI to interact with customers. The chatbot occasionally generates responses that are inappropriate or offensive, leading to customer dissatisfaction. As the AI developer, how do you take responsibility for these incidents and ensure accountability in the deployment of the AI chatbot?\n\nTo ensure accountability and responsible AI deployment, I would take the following steps to mitigate inappropriate or offensive chatbot responses:\n\n1. Implement Robust Content Filtering & Moderation\nIntegrate real-time content moderation tools to detect and filter inappropriate language.\nUse keyword-based filters and machine learning classifiers to flag harmful responses before they reach customers.\n2. Train the AI with Ethical Guidelines\nFine-tune the chatbot with reinforcement learning from human feedback (RLHF) to align responses with ethical and professional standards.\nIncorporate ethical AI principles and diversity-aware datasets to reduce bias and offensive outputs.\n3. Enhance Human Oversight & Intervention\nImplement a human-in-the-loop system where flagged responses are reviewed by customer support teams.\nProvide a seamless escalation mechanism, allowing users to report harmful interactions for human review.\n4. Ensure Transparency & User Control\nClearly communicate that the chatbot is AI-powered and may have limitations.\nOffer customers an option to switch to a human agent when they encounter unsatisfactory responses.\n5. Establish a Continuous Monitoring & Feedback Loop\nContinuously analyze chatbot interactions using AI monitoring dashboards to detect patterns of inappropriate responses.\nUse customer feedback to refine the chatbotâ€™s language model and response moderation.\n6. Conduct Regular Ethical Audits & Compliance Checks\nPeriodically audit chatbot conversations to ensure they align with company policies and ethical AI standards.\nStay compliant with AI regulations (e.g., GDPR, AI Ethics Guidelines) to ensure responsible AI deployment.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Exercise 3:\nYour company has developed a generative AI model that autonomously generates product descriptions for an e-commerce platform. However, users have reported instances where the generated descriptions contain inaccurate information, leading to customer confusion and dissatisfaction. How do you enhance the reliability of your AI model to ensure accurate product descriptions?\n\n",
      "metadata": {}
    }
  ]
}