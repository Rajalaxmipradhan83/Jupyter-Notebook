{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a695874-3f29-46a2-a465-8f4d15071455",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final Project: Building a Rainfall Prediction Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f801cc44-7de3-48d8-bcd3-a428666ff838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0 pyparsing-3.2.1\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.2.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m134.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.12/site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d17ad245-9dcc-4266-a456-28d7119311f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 56420 entries, 6049 to 142302\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Date           56420 non-null  object \n",
      " 1   Location       56420 non-null  object \n",
      " 2   MinTemp        56420 non-null  float64\n",
      " 3   MaxTemp        56420 non-null  float64\n",
      " 4   Rainfall       56420 non-null  float64\n",
      " 5   Evaporation    56420 non-null  float64\n",
      " 6   Sunshine       56420 non-null  float64\n",
      " 7   WindGustDir    56420 non-null  object \n",
      " 8   WindGustSpeed  56420 non-null  float64\n",
      " 9   WindDir9am     56420 non-null  object \n",
      " 10  WindDir3pm     56420 non-null  object \n",
      " 11  WindSpeed9am   56420 non-null  float64\n",
      " 12  WindSpeed3pm   56420 non-null  float64\n",
      " 13  Humidity9am    56420 non-null  float64\n",
      " 14  Humidity3pm    56420 non-null  float64\n",
      " 15  Pressure9am    56420 non-null  float64\n",
      " 16  Pressure3pm    56420 non-null  float64\n",
      " 17  Cloud9am       56420 non-null  float64\n",
      " 18  Cloud3pm       56420 non-null  float64\n",
      " 19  Temp9am        56420 non-null  float64\n",
      " 20  Temp3pm        56420 non-null  float64\n",
      " 21  RainToday      56420 non-null  object \n",
      " 22  RainTomorrow   56420 non-null  object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 10.3+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7557 entries, 64191 to 80997\n",
      "Data columns (total 23 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Date           7557 non-null   object \n",
      " 1   Location       7557 non-null   object \n",
      " 2   MinTemp        7557 non-null   float64\n",
      " 3   MaxTemp        7557 non-null   float64\n",
      " 4   Rainfall       7557 non-null   float64\n",
      " 5   Evaporation    7557 non-null   float64\n",
      " 6   Sunshine       7557 non-null   float64\n",
      " 7   WindGustDir    7557 non-null   object \n",
      " 8   WindGustSpeed  7557 non-null   float64\n",
      " 9   WindDir9am     7557 non-null   object \n",
      " 10  WindDir3pm     7557 non-null   object \n",
      " 11  WindSpeed9am   7557 non-null   float64\n",
      " 12  WindSpeed3pm   7557 non-null   float64\n",
      " 13  Humidity9am    7557 non-null   float64\n",
      " 14  Humidity3pm    7557 non-null   float64\n",
      " 15  Pressure9am    7557 non-null   float64\n",
      " 16  Pressure3pm    7557 non-null   float64\n",
      " 17  Cloud9am       7557 non-null   float64\n",
      " 18  Cloud3pm       7557 non-null   float64\n",
      " 19  Temp9am        7557 non-null   float64\n",
      " 20  Temp3pm        7557 non-null   float64\n",
      " 21  RainYesterday  7557 non-null   object \n",
      " 22  RainToday      7557 non-null   object \n",
      "dtypes: float64(16), object(7)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "#Load the data\n",
    "url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/_0eYOqji3unP1tDNKWZMjg/weatherAUS-2.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n",
    "df.count()\n",
    "\n",
    "#Drop all rows with missing values\n",
    "df = df.dropna()\n",
    "df.info()\n",
    "df.columns\n",
    "\n",
    "#Data leakage considerations\n",
    "df = df.rename(columns={'RainToday': 'RainYesterday',\n",
    "                        'RainTomorrow': 'RainToday'\n",
    "                        })\n",
    "\n",
    "#Location selection\n",
    "df = df[df.Location.isin(['Melbourne','MelbourneAirport','Watsonia',])]\n",
    "df. info()\n",
    "\n",
    "#Extracting a seasonality feature\n",
    "#Create a function to map dates to seasons\n",
    "def date_to_season(date):\n",
    "    month = date.month\n",
    "    if (month == 12) or (month == 1) or (month == 2):\n",
    "        return 'Summer'\n",
    "    elif (month == 3) or (month == 4) or (month == 5):\n",
    "        return 'Autumn'\n",
    "    elif (month == 6) or (month == 7) or (month == 8):\n",
    "        return 'Winter'\n",
    "    elif (month == 9) or (month == 10) or (month == 11):\n",
    "        return 'Spring'\n",
    "\n",
    " #Exercise 1: Map the dates to seasons and drop the Date column\n",
    "    # Convert the 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Apply the function to the 'Date' column\n",
    "df['Season'] = df['Date'].apply(date_to_season)\n",
    "\n",
    "df=df.drop(columns= ['Date'])\n",
    "df\n",
    "\n",
    "#Exercise 2. Define the feature and target dataframes\n",
    "X = df.drop(columns='RainToday', axis=1)\n",
    "y = df['RainToday']\n",
    "\n",
    "#Exercise 3. How balanced are the classes?\n",
    "y.value_counts()\n",
    "\n",
    "#Exercise 4. What can you conclude from these counts?\n",
    "#How often does it rain annualy in the Melbourne area?\n",
    "#How accurate would you be if you just assumed it won't rain every day?\n",
    "#Is this a balanced dataset?\n",
    "\n",
    "#Exercise 5. Split data into training and test sets, ensuring target stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "#Exercise 6. Automatically detect numerical and categorical columns and assign them to separate numeric and categorical features\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()  \n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Scale the numeric features\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "\n",
    "# One-hot encode the categoricals \n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "#Exercise 7. Combine the transformers into a single preprocessing column transformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Exercise 8. Create a pipeline by combining the preprocessing with a Random Forest classifier¶\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "#Define a parameter grid to use in a cross validation grid search model optimizer\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}\n",
    "#Select a cross-validation method, ensuring target stratification during validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#Exercise 9. Instantiate and fit GridSearchCV to the pipeline\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy', verbose=2)  \n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Print the best parameters and best crossvalidation score\n",
    "print(\"\\nBest parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "#Exercise 10. Display your model's estimated score\n",
    "test_score = grid_search.score(X_test, y_test)  \n",
    "print(\"Test set score: {:.2f}\".format(test_score))\n",
    "\n",
    "#Exercise 11. Get the model predictions from the grid search estimator on the unseen data¶\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "#Exercise 12. Print the classification report\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Exercise 13. Plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "#Exercise 14. Extract the feature importances\n",
    "feature_importances = grid_search.best_estimator_['classifier'].feature_importances_\n",
    "\n",
    "# Combine numeric and categorical feature names\n",
    "feature_names = numeric_features + list(grid_search.best_estimator_['preprocessor']\n",
    "                                        .named_transformers_['cat']\n",
    "                                        .named_steps['onehot']\n",
    "                                        .get_feature_names_out(categorical_features))\n",
    "\n",
    "feature_importances = grid_search.best_estimator_['classifier'].feature_importances_\n",
    "\n",
    "N = 20  # Change this number to display more or fewer features\n",
    "top_features = importance_df.head(N)\n",
    "\n",
    "importance_df = pd.DataFrame({'Feature': feature_names,\n",
    "                              'Importance': feature_importances\n",
    "                             }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the most important feature on top\n",
    "plt.title(f'Top {N} Most Important Features in predicting whether it will rain today')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Exercise 15. Update the pipeline and the parameter grid\n",
    "# Replace RandomForestClassifier with LogisticRegression\n",
    "pipeline.set_params(classifier=LogisticRegression(random_state=42))\n",
    "\n",
    "# Update the model's estimator to use the new pipeline\n",
    "grid_search.estimator = pipeline\n",
    "\n",
    "# Define a new grid with Logistic Regression parameters\n",
    "param_grid = {\n",
    "    'classifier__solver': ['liblinear'],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid_search.param_grid = param_grid\n",
    "\n",
    "# Fit the updated pipeline with LogisticRegression\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "#Compare the results to your previous model.\n",
    "#Display the clasification report and the confusion matrix for the new model and compare your results with the previous model.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Generate the confusion matrix \n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Titanic Classification Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe9e36-d799-4a81-bcfe-6f2d5da0370f",
   "metadata": {},
   "source": [
    "Exercise 4. What can you conclude from these counts?\n",
    "How often does it rain annualy in the Melbourne area?\n",
    "How accurate would you be if you just assumed it won't rain every day?\n",
    "Is this a balanced dataset?\n",
    "1.Resampling: If the dataset is imbalanced, you might need to consider resampling techniques, such as oversampling the minority class (rain) or undersampling the majority class (no rain).\n",
    "2.Model Evaluation: Use metrics like precision, recall, and F1-score, which are more informative than accuracy in imbalanced datasets.\n",
    "3.Feature Engineering: Investigate additional features that might be useful in improving the model (e.g., weather conditions like temperature, humidity).\n",
    "4.Model Selection: Test different algorithms (e.g., Random Forest, Logistic Regression) and compare performance.\n",
    "5.Cross-validation: Use techniques like cross-validation to get a more robust estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7eecc0-e63e-4c07-82ae-072d22479020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
